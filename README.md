# Traffic Light Recognition Using Deep Learning

## Рассмотренные датасеты:
* [Lisa Traffic Lights Dataset](https://www.kaggle.com/mbornoe/lisa-traffic-light-dataset)
* * Был использован как основной датасет, т.к. не требовал большой работы по очистке и преобразованию данных, понятных нейросети
* [DriveU Traffic Light Dataset (DTLD)](https://www.uni-ulm.de/en/in/driveu/projects/driveu-traffic-light-dataset/)
* * Отказались по причине необходимости писать парсинги для извлечения tiff-файлов и калибрации
* [Bosch Small Traffic Lights Dataset](https://hci.iwr.uni-heidelberg.de/content/bosch-small-traffic-lights-dataset)
* * Отсутствие внятной документации
  
## Статьи
* [Traffic Light Detection with ConvolutionalNeural Networks and 2D Camera Data](https://www.mi.fu-berlin.de/inf/groups/ag-ki/Theses/Completed-theses/Bachelor-theses/2020/Hein/BA-Hein.pdf)
* https://github.com/level5-engineers/system-integration/wiki/Traffic-Lights-Detection-and-Classification
* https://arxiv.org/pdf/1906.11886.pdf
* A Deep Analysis of the Existing Datasets for Traffic Light State Recognition


  
## Комментарии, эксперименты, процесс работы
1. В сравнении кодеров-декодеров видео очевидно более быстрой оказалась утилита ffmpeg, в папку modules были добавлен data unpacker.
2. LISA была размечена при помощи pandas, были убраны все ненужные параметры (например, на какое видео ссылается кадр). Итоговый датафрейм состоял из frame_id, уникального идентификатора кадра, путь к кадру, бокс светофора, и его состояние (можно найти в datasets/final.csv)
3. Первоначальной идеей для модели предсказания было взять уже предобученную CNN, используя Torch Hub. Изучив SOTA в отношении детекции обьектов, мы решили обратить внимание на R-CNN, а конкретно, Faster R-CNN, она имелась в хабе, а значит ее несложно было заимплементить в общий проект, к тому же, была одной из лучших по точности предсказаний. Также, запасным вариантом была YOLO, т.к. была достаточно быстрой и практически везде всплывала. Был дописан переход от боксов Lisa к YOLO, (одна ошибка в котором немного позже стоила нам недели работы). Параметры (x_min, y_min, x_max, y_max) преобразовывались в координаты центра бокса(x, y, ширины width и высоты height).
4. Впоследствии выяснилось, что Faster R-CNN совсем не укладывается в необходимое время работы системы, несмотря на наши попытки изменять разрешение кадра и вырезать участки, где расположены большинство светофоров на кадрах.
5. После неудачи с Faster R-CNN, мы вернулись к YOLO. Было принято решение всё же собрать архитектуру с нуля по туториалам, используя [это](https://github.com/aladdinpersson/Machine-Learning-Collection), дабы иметь больший контроль над системой. Однако, обработку output'а YOLO, а именно поиск наилучшего бокса,мы решили взять с другой имплементации. Совместить их было гораздо труднее, чем мы ожидали, и впоследствии сменили её на готовую модель. 
6. Вот тут и всплыла ошибка, описанная на 3-м шаге. Наша текущая на момент реализация YOLO при обучении, в абсолютно случайных местах в датасете выдавала ошибку, что боксы светофоров больше, чем сам кадр (все параметры вывода боксов после yolo не должны выходить за интервал [0, 1] Конечно, проверка боксов в датасете показывала, что все нормально. После длительного перерыва и более глубокого разбора, мы нашли проблему: в текущей имплементации, для аугментаций использовался модуль albumentations, в котором немного странно работает нормализация боксов для YOLO, часть форумов об этом писала. Оказалось, что при делении боксов на разрешение, чтобы получить интервал [0, 1], при нахождении width и height боксов необходимо было не только делить на 2, но и вычитать один пиксель. Иначе Albumentations не принимал наши боксы. Временным решением, пока мы не узнали в чем проблема, было подкорректировать исходный код мудуля augmentations, отвечающего за боксы, bbox_utils, дабы он все боксы, при аугментации выходившие за интервал, возвращал до нуля или единицы.
7. В целом, эта реализация имела и ряд других проблем, поэтому, мы решели все же снова сменить имплементацию модели на [эту](https://github.com/ultralytics/yolov3), ставшую уже последней.
8. Изменив необходимые параметры для загрузки нашего датасета (в данной имплементации, они выполняются через .yaml-файлы), заморозив backbone-часть сети (а первоначально, мы этот шаг пропустили, за что, естественно, пожалели) мы попытались поучить сеть на домашнем ПК с GTX960 2GB, конечно же, не уложившись по памяти.
9. Паралелльно, были размечены кадры с папки проекта phase_I вручную, используя [YOLO Annotation Tool](https://github.com/tunahansalih/yolo-annotation-tool). Размеченные 600 кадров были добавлены в валидационную выборку.
10. Вернувшись в colab, всё же правильно настроив все параметры, мы начали процесс обучения, но за 20 эпох толкового результата не оказалось. Необходимо было что-то изменить. И возникла интересная, но простая идея: взять обученную YOLO на датасете MS COCO, т.к. там уже присутствует класс 'Traffic_light', и дообучить сеть на датасете LISA. Было дообучено 10 эпох.
11. Так как по итогу наша сетка размечает светофоры, выделяя непосредственно кружок активного состояния светофора, мы решили определять цвет средствами open-cv. Для этого мы кропаем бокс светофора (кружка), переводим ихображение в формат HSV, что позволяет нам подобрать экспериментально диапазон значений в формате HSV, которые соответствуют каждому из цветов, в случае неопознанного цвета (не желтого/красного/зеленого) мы выводим значение unknown. Получились довольно хорошие результаты. Более того, если наша сетка делает ошибку, в ходе которой размечается объект с цветом отличным от зеленого/красного/желтого, классификатор определяет объект как unknown, что тем самым не вредит метрике.
12. Основая проблема нашей сетки - это мигания цвета и выбросы, которые мы решили с помощью постобработки, фактически используем скользящее среднее для сглаживания мигания

# Структура проекта
* В папке datasets находится первоначальный ноутбук обработки LISA, а также метаданные.
* В modules: ранние декодироващик и кодировщик видео, а также класс Dataset для работы с torch.
* app.py - планировали сделать визуализацию результатов через streamlit, но отказались
* yolov3 хранит основые файлы проекта:
* * в папке data - скрипты загрузки датасета MS COCO, yaml-файлы для разметки данных, включая наш - traffic.yaml
* * models содержит архитектуру yolov3 и её файлы конфигурации, а также код, позволяющий преобразовывать выходные данные модели в боксы
* * в utils находятся файлы подсчета лосса, метрик, функции для визуализации, работа с датасетами и пр.
* * weights содержит скрипт, который позволяет подгружать веса предобученной YOLO.
* * train.py и test.py запускают процесс обучения нейросети и её валидации соответственно, detect_custom.py позволяет визуализировать результаты работы.
